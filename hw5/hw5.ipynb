{
 "metadata": {
  "name": "",
  "signature": "sha256:0bc9ddc807d8c9518af43991e81655a0715e5a988c3f23eaca60e6ddf5d5dbf9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Homework 5\n",
      "==========\n",
      "\n",
      "- Name: Austin\n",
      "- SID: 23826762\n",
      "- Repro: Open up hw5.ipynb in IPython Notebook.\n",
      "\n",
      "## a) Techniques implemented\n",
      "\n",
      "The stopping criteria for my decision trees fall into 2 terminating cases. First, if all the remaining data shares a single label, make a leaf node. Second, if the depth is hit at any node, then the most common label is chosen for that leaf node.\n",
      "\n",
      "In terms of splitting heuristics, I tried median of a feature, mean of a feature, and the mean of the mean of all features, all of which seemed to perform similarly, because they almost always spllt between 0 and everything else. For speedups, I tried picking random features to split on instead of testing all features, which expectably led to a decrease of a few percent in accuracy.\n",
      "\n",
      "To test the effect of various hyperparameters (mostly DTree depth and RForest size), I used 12-fold cross validation.\n",
      "\n",
      "In my random forests, I implemented bagging of the data in addition to random subsets of the features at each node.\n",
      "\n",
      "## b) Features added or removed\n",
      "\n",
      "I didn't add or remove any features =P.\n",
      "\n",
      "## c) Results\n",
      "\n",
      "On my decision tree, with depth 15 and mean of features, I get about 0.825 accuracy on my training data through cross-validation, and a kaggle score of 0.79269\n",
      "\n",
      "On my random forests, with depth 12 and 24 trees, I get about 0.835 accuracy on my training data through cross-validation, and a kaggle score of 0.81523.\n",
      "\n",
      "## d) Example classification on DTree\n",
      "\n",
      "After training a depth 15 DTree on all the data, this is an example traversal on the first test data point.\n",
      "\n",
      "    feature 28 (!):     1.0 > 0.703209590101\n",
      "    feature 19 (meter): 0.0 <= 0.29561752988\n",
      "    feature 31 (&):     0.0 <= 0.335042735043\n",
      "    feature 3  (money): 0.0 <= 0.183125599233\n",
      "    feature 26 ($):     0.0 <= 1.01434878587\n",
      "\n",
      "This point was classified as 1 (spam).\n",
      "\n",
      "## e) Common top-level splits in RForest\n",
      "\n",
      "Out of 100 DTrees in a Random Forest, the breakdown of first splits is as follows:\n",
      "\n",
      "    17% 28 (!)\n",
      "    16% 19 (meter)\n",
      "    15% 3  (money)\n",
      "    15% 16 (volumes)\n",
      "    10% 6  (prescription)\n",
      "    6% 0\n",
      "    4% 9\n",
      "    3% 29\n",
      "    2% 5\n",
      "    2% 4\n",
      "    2% 26\n",
      "    1% 1\n",
      "    1% 10\n",
      "    1% 12\n",
      "    1% 18\n",
      "    1% 27\n",
      "    1% 31\n",
      "    1% 7\n",
      "    \n",
      "All of the splits were thresholded by 0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy.io\n",
      "import math\n",
      "import random\n",
      "\n",
      "# Load the data\n",
      "mat = scipy.io.loadmat('spam-dataset/spam_data.mat')\n",
      "training_data = mat['training_data']\n",
      "training_labels = mat['training_labels']\n",
      "training_labels = np.squeeze(np.asarray(training_labels))\n",
      "test_data = mat['test_data']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Node(object):\n",
      "    def __init__(self, split_rule, left, right):\n",
      "        self.split_rule = split_rule\n",
      "        self.left = left\n",
      "        self.right = right\n",
      "        \n",
      "    def __repr__(self, level=0):\n",
      "        ret = \"- \"*level + str(self.split_rule) + \"\\n\"\n",
      "        ret += self.left.__repr__(level + 1)\n",
      "        ret += self.right.__repr__(level + 1)\n",
      "        return ret\n",
      "    \n",
      "class LeafNode(object):\n",
      "    def __init__(self, label):\n",
      "        self.label = label\n",
      "    \n",
      "    def __repr__(self, level=0):\n",
      "        return \"- \"*level + str(self.label) + \"\\n\"\n",
      "        \n",
      "class DTree(object):\n",
      "    def __init__(self, depth, impurity, segmentor):\n",
      "        self.depth = depth\n",
      "        self.impurity = impurity\n",
      "        self.segmentor = segmentor\n",
      "    \n",
      "    def train(self, data, labels):\n",
      "        self.root = self.growTree(data, labels, self.depth)\n",
      "    \n",
      "    def growTree(self, data, labels, depth):\n",
      "        if depth == 0:\n",
      "            return LeafNode(1 if sum(labels) > len(labels) / 2 else 0)\n",
      "        \n",
      "        # Base case: if all labels are 0\n",
      "        if sum(labels) == 0:\n",
      "            return LeafNode(0)\n",
      "        \n",
      "        # Base case: if all labels are 1\n",
      "        if sum(labels) == len(labels):\n",
      "            return LeafNode(1)\n",
      "        \n",
      "        # Recursive case:\n",
      "        split, threshold = self.segmentor(data, labels, self.impurity)\n",
      "        lp = np.array([f <= threshold for f in data.T[split]])\n",
      "        rp = np.array([f > threshold for f in data.T[split]])\n",
      "        \n",
      "        left_node = self.growTree(data[lp], labels[lp], depth - 1)\n",
      "        right_node = self.growTree(data[rp], labels[rp], depth - 1)\n",
      "        return Node((split, threshold), left_node, right_node)\n",
      "\n",
      "    # Traverse through the D-Tree for this data point\n",
      "    def traverse(self, datum):\n",
      "        node = self.root\n",
      "        while type(node) is not LeafNode:\n",
      "            split, threshold = node.split_rule\n",
      "            node = node.left if datum[split] <= threshold else node.right\n",
      "            \n",
      "        return node.label\n",
      "    \n",
      "    def predict(self, data):\n",
      "        result = [self.traverse(datum) for datum in data]\n",
      "        return result\n",
      "    \n",
      "    # Returns the proportion of data correctly labelled by predict\n",
      "    def score(self, data, labels):\n",
      "        predictions = self.predict(data)\n",
      "        return sum(d == l for d, l in zip(predictions, labels)) / len(labels)\n",
      "\n",
      "def permute_both(a, b):\n",
      "    p = np.random.permutation(len(a))\n",
      "    return a[p], b[p]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Various impurity and segmentor functions\n",
      "def entropy(left, right):\n",
      "    if left == 0 or right == 0: return 0\n",
      "    p_left, p_right = left / (left + right), right / (left + right)\n",
      "    return -p_left * np.log2(p_left) + -p_right * np.log2(p_right)\n",
      "\n",
      "def entropy_impurity(left_hist, right_hist):\n",
      "    result = sum(left_hist) * entropy(*left_hist) + sum(right_hist) * entropy(*right_hist)\n",
      "    return result / (sum(left_hist) + sum(right_hist))\n",
      "\n",
      "def impurity_func(data, labels, impurity):\n",
      "    def result(split_rule):\n",
      "        feature = data.T[split_rule[0]]\n",
      "        threshold = split_rule[1]\n",
      "\n",
      "        a, b, c, d = 0, 0, 0, 0\n",
      "        for label, f in zip(labels, feature):\n",
      "            if f <= threshold:\n",
      "                if label == 1:\n",
      "                    c += 1\n",
      "                else:\n",
      "                    d += 1\n",
      "            else:\n",
      "                if label == 1:\n",
      "                    a += 1\n",
      "                else:\n",
      "                    b += 1\n",
      "        \n",
      "        return impurity((a, b), (c, d))\n",
      "    return result\n",
      "\n",
      "def median_segmentor(data, labels, impurity):\n",
      "#     split_rules = [(i, np.median(feature)) for i, feature in enumerate(data.T)]\n",
      "    split_rules = [(i, np.mean(np.mean(data, 1))) for i, feature in enumerate(data.T)]\n",
      "    return min(split_rules, key=impurity_func(data, labels, impurity))\n",
      "\n",
      "from random import randint\n",
      "def random_segmentor(data, labels, impurity):\n",
      "    i = randint(0, len(data.T)-1)\n",
      "#     print(np.mean(np.mean(data, 1)))\n",
      "    return (i, np.mean(np.mean(data, 1)))\n",
      "\n",
      "def forest_segmentor(data, labels, impurity):\n",
      "    split_rules = [(i, np.mean(np.mean(data, 1))) for i, feature in enumerate(data.T)]\n",
      "    # Random subset of 6 splits at each node (roughly sqrt(32))\n",
      "    split_rules = random.sample(split_rules, 6)\n",
      "    \n",
      "    return min(split_rules, key=impurity_func(data, labels, impurity))\n",
      "\n",
      "def all_segmentor(data, labels, impurity):\n",
      "    features = np.random.choice(32, 6, replace=False)\n",
      "    split_rules = {(fe, i) for fe in features for i in data.T[fe]}\n",
      "    return min(split_rules, key=impurity_func(data, labels, impurity))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Random forests\n",
      "\n",
      "class RForest(object):\n",
      "    def __init__(self, depth, impurity, segmentor, num):\n",
      "        self.trees = [DTree(depth, impurity, segmentor) for _ in range(num)]\n",
      "        \n",
      "    def train(self, dataX, labelsX):\n",
      "        for tree in self.trees:\n",
      "            # Bagging\n",
      "            r = np.random.choice(len(dataX), int(len(dataX) / 2))\n",
      "            data, labels = dataX[r], labelsX[r]\n",
      "            tree.train(data, labels)\n",
      "        \n",
      "    def predict(self, data):\n",
      "        a = [tree.predict(data) for tree in self.trees]\n",
      "        result = []\n",
      "        for i in range(len(data)):\n",
      "            total = sum(guess[i] for guess in a)\n",
      "            result.append(1 if total > (len(self.trees) / 2) else 0)\n",
      "        return result\n",
      "                \n",
      "        \n",
      "    # Returns the proportion of data correctly labelled by predict\n",
      "    def score(self, data, labels):\n",
      "        predictions = self.predict(data)\n",
      "        return sum(d == l for d, l in zip(predictions, labels)) / len(labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 137
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cross_validate():\n",
      "    l = np.squeeze(np.asarray(training_labels))\n",
      "    d, l = permute_both(training_data, training_labels)\n",
      "\n",
      "    # Cross validation to test how good this D-Tree is\n",
      "    folds = 12\n",
      "    interval = 431\n",
      "\n",
      "    total = 0\n",
      "    # cross validation\n",
      "    for k in range(folds):\n",
      "        kemails, klabels = [], []\n",
      "        tree = RForest(12, entropy_impurity, forest_segmentor, 24)\n",
      "#         tree = DTree(15, entropy_impurity, median_segmentor)\n",
      "#         print(tree.root)\n",
      "        for i in range(folds):\n",
      "            if i != k:\n",
      "                start, end = i * interval, (i + 1) * interval\n",
      "                kemails.append(d[start:end])\n",
      "                klabels.append(l[start:end])\n",
      "        tree.train(np.concatenate(kemails), np.concatenate(klabels))\n",
      "\n",
      "        start, end = k * interval, (k + 1) * interval\n",
      "        total += tree.score(d[start:end], l[start:end])\n",
      "        print(total / (k + 1))\n",
      "\n",
      "    print(total / folds)\n",
      "\n",
      "%prun cross_validate()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Train the DTree on all the training data\n",
      "d, l = permute_both(training_data, training_labels)\n",
      "tree = RForest(12, entropy_impurity, forest_segmentor, 24)\n",
      "tree.train(d, l)\n",
      "\n",
      "# Predict the labels for the test data\n",
      "result = tree.predict(test_data)\n",
      "\n",
      "# Write the results to a csv\n",
      "f = open('spam5.csv', 'w')\n",
      "f.write('Id,Category\\n')\n",
      "for i in range(len(result)):\n",
      "    f.write(\"{0},{1}\\n\".format(i + 1, result[i]))\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 132
    }
   ],
   "metadata": {}
  }
 ]
}
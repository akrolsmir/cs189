{
 "metadata": {
  "name": "",
  "signature": "sha256:d0a6b2cefc1ef58d1cb2b76140db2d7f0891fb03cbc0e9a6cb401c389eef70bf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Square brackets denote element-wise operations.\n",
      "\n",
      "### Mean-squared error\n",
      "$$J = 1/2 \\sum (y_k - h_{k}(x))^2$$\n",
      "$$dJ / dW_2 = - \\sum (y_k - h_{k}(x)) * d/dW_2 (h_{k}(x))$$\n",
      "\n",
      "Note that $h(x) = g(tanh(xW_1)W_2)$, where $x$ is a row vector. Consider $d/dW_2 (h(x))$:\n",
      "\n",
      "$$d/dW_2 (h(x)) = d/dW_2 (tanh(xW_1)W_2) * g'(tanh(xW_1)W_2)$$\n",
      "$$ = tanh(xW_1)^T * [g(tanh(xW_1)W_2) (1 - g(tanh(xW_1)W_2))]$$\n",
      "$$ = tanh(xW_1)^T * [h(x) (1 - h(x))]$$\n",
      "\n",
      "Thus:\n",
      "$$dJ / dW_2 = - tanh(xW_1)^T * [h(x) (1 - h(x))(y - h(x))] $$\n",
      "\n",
      "### \n",
      "\n",
      "$$dJ / dW_1 = - \\sum (y_k - h_{k}(x)) * d/dW_1 (h_{k}(x))$$\n",
      "Consider $d/dW_1 (h(x))$:\n",
      "\n",
      "$$d/dW_1 (h(x)) = d/dW_1 (tanh(xW_1)W_2) * g'(tanh(xW_1)W_2)$$\n",
      "$$d/dW_1 (tanh(xW_1)W_2) = W_2^T tanh'(xW_1)x^T$$\n",
      "\n",
      "Thus:\n",
      "$$dJ / dW_2 = - x^T * [[h(x)(1 - h(x))(y - h(x))] * W_2^T (tanh'(xW_1))] $$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy.io\n",
      "import math\n",
      "import random\n",
      "from sklearn import preprocessing\n",
      "\n",
      "# Load the data\n",
      "mat = scipy.io.loadmat('digit-dataset/train.mat')\n",
      "images = np.reshape(mat[\"train_images\"], (1, -1, 60000))[0].T\n",
      "labels = mat['train_labels']\n",
      "labels = np.squeeze(np.asarray(train_labels))\n",
      "\n",
      "# Preprocess X\n",
      "X = preprocessing.scale(images.astype(float))\n",
      "\n",
      "# Form y row vectors\n",
      "Y = [[1 if label == i else 0 for i in range(10)] for label in labels]\n",
      "Y = np.array(Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 148
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.shape(np.random.normal(size=(784, 200)))\n",
      "\n",
      "t = np.array([[1, 2],[3, 4]])\n",
      "a = np.array([1, 2, 3])\n",
      "b = np.array([3, 4, 5])\n",
      "np.shape(a)\n",
      "np.shape(b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "(3,)"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNet(object):\n",
      "    def __init__(self):\n",
      "        e = 10**-5\n",
      "        self.W1 = np.random.normal(0, e, (784, 200))\n",
      "        self.W2 = np.random.normal(0, e, (200, 10))\n",
      "        \n",
      "#         self.W1 = np.ones((784, 200)) * 0.0002\n",
      "#         self.W2 = np.ones((200, 10)) * 0.03\n",
      "    \n",
      "    def forward(self, x):\n",
      "        hx = sigmoid(np.tanh(x.dot(self.W1)).dot(self.W2))\n",
      "        return hx\n",
      "    \n",
      "    def train(self, X, Y):\n",
      "        for i in range(40000):\n",
      "            i = random.randrange(0, len(X))\n",
      "            x, y = X[i], Y[i] # x and y are both row vectors\n",
      "\n",
      "            a2 = np.tanh(x.dot(self.W1))\n",
      "            hx = sigmoid(a2.dot(self.W2))\n",
      "            d3 = np.multiply(np.multiply(hx, 1 - hx), y - hx)\n",
      "\n",
      "            dJdW2 = - np.outer(a2.T, d3)\n",
      "#             print(\"DJDW2: \", dJdW2[9][9])\n",
      "#             print(\"numerical grad\", self.numerical_gradient(x, y))\n",
      "\n",
      "            dJdW1 = - np.outer(x.T, d3.dot(self.W2.T) * (1 - a2**2))\n",
      "#             print(\"DJDW1: \", dJdW1[9][9])\n",
      "#             print(\"numerical grad\", self.num_grad_W1(x, y))\n",
      "\n",
      "            step = 0.0001\n",
      "            self.W2 -= step * dJdW2\n",
      "            self.W1 -= step * dJdW1\n",
      "    \n",
      "    def predict(self, images):\n",
      "        output = self.forward(images)\n",
      "        return [max((v, i) for i, v in enumerate(o))[1] for o in output]\n",
      "        \n",
      "    def score(self, data, labels):\n",
      "        predictions = self.predict(data)\n",
      "        return sum(d == l for d, l in zip(predictions, labels)) / len(labels)\n",
      "        \n",
      "    def numerical_gradient(self, x, y):\n",
      "        e = 10**-5\n",
      "        perturb = np.zeros(self.W2.shape)\n",
      "        perturb[9][9] = e\n",
      "        \n",
      "        Wp = self.W2 + perturb\n",
      "        a2 = np.tanh(x.dot(self.W1))\n",
      "        hx = sigmoid(a2.dot(Wp))\n",
      "        error = y - hx\n",
      "        Jp = 0.5 * error.dot(error)\n",
      "        \n",
      "        Wm = self.W2 - perturb\n",
      "        a2 = np.tanh(x.dot(self.W1))\n",
      "        hx = sigmoid(a2.dot(Wm))\n",
      "        error = y - hx\n",
      "        Jm = 0.5 * error.dot(error)\n",
      "        \n",
      "        return (Jp - Jm) / (2 * e)\n",
      "    \n",
      "    def num_grad_W1(self, x, y):\n",
      "        e = 10**-5\n",
      "        perturb = np.zeros(self.W1.shape)\n",
      "        perturb[9][9] = e\n",
      "        \n",
      "        Wp = self.W1 + perturb\n",
      "        a2 = np.tanh(x.dot(Wp))\n",
      "        hx = sigmoid(a2.dot(self.W2))\n",
      "        error = y - hx\n",
      "        Jp = 0.5 * error.dot(error)\n",
      "        \n",
      "        Wm = self.W1 - perturb\n",
      "        a2 = np.tanh(x.dot(Wm))\n",
      "        hx = sigmoid(a2.dot(self.W2))\n",
      "        error = y - hx\n",
      "        Jm = 0.5 * error.dot(error)\n",
      "        \n",
      "        return (Jp - Jm) / (2 * e)\n",
      "        \n",
      "def sigmoid(z):\n",
      "    return 1 / (1 + np.exp(-z))\n",
      "\n",
      "nn = NeuralNet()\n",
      "# nn.forward(np.ones(784))\n",
      "%prun nn.train(X, Y)\n",
      "nn.score(X, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " "
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 167,
       "text": [
        "0.57315000000000005"
       ]
      }
     ],
     "prompt_number": 167
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sigmoid(np.array([-10, -0.5, 0, 0.5, 10]))\n",
      "\n",
      "while():\n",
      "    print(\"HI\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 142
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 146
    }
   ],
   "metadata": {}
  }
 ]
}